{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Template_Language_Model_Vincent_Dandenault.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdandenault/AzureMLDeploy/blob/main/Template_Language_Model_Vincent_Dandenault.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "vyqbtmU30Q5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import optax\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(42)"
      ],
      "metadata": {
        "id": "Sa_ddfY2SEkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to learn a [basic language model](https://en.wikipedia.org/wiki/Language_model) using an recurrent neural network. \n",
        "\n",
        "As as starting point, you should implement a simple RNN of the form (for reference, see the Deep Learning textbook chapter 10 page 370): \n",
        "\\begin{align*}\n",
        "a^{(t)} &= Wh^{(t-1)} + Ux^{(t)} + b \\\\\n",
        "h^{(t)} &= \\tanh(a^{(t)})\\\\\n",
        "o^{(t)} &= Vh^{(t)} + c \\enspace ,\n",
        "\\end{align*}\n",
        "where $h^{(t)}$ is the updated state at time $t$ , $x^{(t)}$ is the input and $o^{(t)}$ is the output. Given an initial input $x^{(0)}$ and hidden state $h^{(0)}$, an RNN computes the output sequence $o^{(0)}, ..., o^{(T)}$ by applying $f$ recursively: \n",
        "\\begin{align*}\n",
        "(h^{(t+1)}, o^{(t+1)}) = f(h^{(t)}, x^{(t)}; \\theta) \\enspace .\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "RAWCIVnxFLdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ElmanCell(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, state, x):\n",
        "    x = jnp.concatenate([state, x])\n",
        "    new_state = jnp.tanh(nn.Dense(state.shape[0])(x))\n",
        "    return new_state\n",
        "    # IMPLEMENT the basic RNN cell described above (outputting h^{t} only)"
      ],
      "metadata": {
        "id": "i6xzFD-DSKrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutiple such cells can be chained up together to attain more expressivity, for example, we can link two cells as follows:\n",
        "\\begin{align*}\n",
        "h_1^{(t+1)} &= \\tanh(W_1h_1^{(t)} + U_1x^{(t)} + b_1)\\\\\n",
        "h_2^{(t+1)} &= \\tanh(W_1h_1^{(t)} + U_1h_1^{(t)} + b_1)\\\\\n",
        "o^{(t+1)} &= Vh_2^{(t+1)} + c \\enspace ,\n",
        "\\end{align*}\n",
        "and the resulting network is of the form: \n",
        "\\begin{align*}\n",
        "(h_1^{(t+1)}, h_2^{(t+1)}, o^{(t+1)}) = f(h_1^{(t)}, h_2^{(t)}, x^{(t)}; \\theta) \\enspace .\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "HzgGOmfNHHKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentNetwork(nn.Module):\n",
        "  state_size: int\n",
        "  num_classes: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, state, i):\n",
        "    x = jnp.squeeze(jax.nn.one_hot(i, self.num_classes))\n",
        "    state = ElmanCell()(state[0], x)\n",
        "    predictions = nn.softmax(nn.Dense(self.state_size)(state))\n",
        "    return (state, ), predictions\n",
        "\n",
        "  def init_state(self):\n",
        "    return (jnp.zeros(self.state_size),)"
      ],
      "metadata": {
        "id": "Y0dqECaIHH10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We learn our language model by taking a written document and learn to predict the next character (a character-level language model) using our RNN. This is prediction task is akin to a classification and we therefore use the cross-entropy loss: \n",
        "\\begin{align*}\n",
        "l(x, y, \\theta) \\triangleq -\\log p_\\theta(y|x) \\enspace.\n",
        "\\end{align*}\n",
        "We compute those probabilities using our RNN. The output $o^{(t)}$ represent the the so-called **logits** which can be transformed into probabilities using the softmax function. That is $\\text{softmax}(o^{(t)})$ gives us the desired probabilities. "
      ],
      "metadata": {
        "id": "PEU-BgnOJj_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_rnn_loss(model):\n",
        "  def cross_entropy(logits, target):\n",
        "    \"\"\" Negative cross-entropy\n",
        "    Args:\n",
        "      logits: jnp.ndarray with num_classes dimensions (unbatched, batching is done outside through vmap)\n",
        "      target (int): target class that should have been predicted\n",
        "    \"\"\"\n",
        "    epsilon=1e-12\n",
        "    logits = jnp.clip(logits, epsilon, 1. - epsilon)\n",
        "    log_p = jax.nn.log_softmax(logits)\n",
        "    return -jnp.sum(target * log_p)/logits.shape[0]\n",
        "    # IMPLEMENT the cross-entropy loss\n",
        "    # Hint: use jax.nn.log_softmax to avoid computing the log and \n",
        "    # the softmax separately. This function is numerically more stable that the naive approach. \n",
        "\n",
        "  def rnn_loss(params, inputs, targets, init_state):\n",
        "    final_state, logits = jax.lax.scan(partial(model.apply, params), init_state, inputs)\n",
        "    loss = jnp.mean(jax.vmap(cross_entropy, in_axes=(0, 0))(logits, targets.astype(jnp.int32)))\n",
        "    return loss, final_state\n",
        "\n",
        "  return jax.jit(rnn_loss)"
      ],
      "metadata": {
        "id": "2EOF-OOBSp-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the above function, we unroll the ``jax.lax.scan`` over a given input sequence and compute the loss along. When it comes to generating new content, we have to execute our RNN differently. That is: we provide a starting hidden state and character $x^{(0)}$, compute the distribution over next character using the softmax transformation of the logits computed as output of the RNN, sample one of those next character, and repeat the process in this manner until we reach a desired length. In other words, we generate a string *auto-regressively*. \n",
        "\n",
        "A variant on the above procedure is to let the RNN start from more than just a given single character and instead pass a longer *prompt*. The same idea holds except that we have to ``jax.lax.scan`` over as many characters as we have in our prompt. \n",
        "\n",
        "The process described above is *stochastic* in nature: the next character is sampled according to the predicted class distribution. When using the softmax transformaiton,  can vary the degree of stochasticity using a temperature parameter $\\tau$. All we have to do is to multiply the logits by the inverse temperature: $\\text{softmax}((1/\\tau)o^{(t)})$. The smaller the temperature, the more deterministic the model becomes. \n"
      ],
      "metadata": {
        "id": "GblzQ6icMKJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    x_prime = jnp.exp(x - jnp.max(x))\n",
        "    return x_prime / x_prime.sum(axis=0)\n",
        "\n",
        "def sample(key, model, params, id_lookup, chr_lookup, prompt='', max_length=100, temperature=1.):  \n",
        "  encoded_prompt = jnp.asarray(list(map(lambda c: id_lookup[c], prompt)))\n",
        "  state, _ = jax.lax.scan(partial(model.apply, params), model.init_state(), encoded_prompt[:-1])\n",
        "\n",
        "  num_classes = len(id_lookup)\n",
        "  def autoregressive_step(carry, subkey):\n",
        "    state, last_char = carry\n",
        "    state, logits = model.apply(params, state, last_char)\n",
        "    # IMPLEMENT # sample the next character at the given temperature\n",
        "    prediction = jnp.amax(softmax(logits/temperature)) #max probability\n",
        "    return (state, prediction), prediction\n",
        "  keys = jax.random.split(key, max_length)\n",
        "  _, sequence = jax.lax.scan(autoregressive_step, (state, id_lookup[prompt[-1]]), xs=keys)\n",
        "  decoded_sequence = list(map(lambda i: chr_lookup[int(i)], sequence))\n",
        "\n",
        "  return prompt + ''.join(decoded_sequence)"
      ],
      "metadata": {
        "id": "ERyDwz3ISids"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code doesn't have to be modified. Its purpose is to turn the text into one-hot vectors (features) and to chunk up the text (which can be very large) into smaller and more manageable subsequences. "
      ],
      "metadata": {
        "id": "U7SfuqpeOEYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(x, seq_size):\n",
        "  if seq_size > x.shape[0]:\n",
        "    return jnp.atleast_2d(x[:-1]), jnp.atleast_2d(x[1:])\n",
        "  num_partitions = int(jnp.ceil(x.shape[0]/seq_size))\n",
        "  inputs = jnp.array_split(x, num_partitions)\n",
        "  targets = jnp.array_split(jnp.append(x[1:], jnp.nan), num_partitions)\n",
        "  return inputs[:x.shape[0] % num_partitions], targets[:x.shape[0] % num_partitions]"
      ],
      "metadata": {
        "id": "oLBRvWeglJuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_subsequence(key, data, size):\n",
        "    ridx = jax.random.randint(key, (1,), minval=0, maxval=data.shape[0]-size)[0]\n",
        "    return data[ridx:ridx+size]"
      ],
      "metadata": {
        "id": "1irXNSaZ1_z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "  unique_chars = set(data)\n",
        "  id_lookup = dict(zip(unique_chars, range(len(unique_chars))))\n",
        "  chr_lookup = dict(zip(range(len(unique_chars)), unique_chars))\n",
        "  encoded_data = jnp.asarray(list(map(lambda c: id_lookup[c], data)))\n",
        "  return encoded_data, id_lookup, chr_lookup, len(unique_chars)"
      ],
      "metadata": {
        "id": "Au_jKTsk28br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the large size of the training sequence (the entire text document), we have to split it into manageable subsequences. More precisely, at every training *epoch* we sample a contiguous subsequence from the entire document and compute the negative log likelihood loss by unrolling the RNN over the given characters. However, given the challenge (more on this in question) of learning over long horizon, we truncate the unroll over fewer characters and warm start the initial state between each such truncated unroll. "
      ],
      "metadata": {
        "id": "7264m-Bs8_UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(key, data, state_size, learning_rate, n_epochs, batch_size, max_subsequence_length, sample_length, test_prompt=None, temperature=1.):\n",
        "  encoded_data, id_lookup, chr_lookup, num_classes = preprocess(data)\n",
        "  model = RecurrentNetwork(state_size, num_classes)\n",
        "  params = model.init(key, model.init_state(), 0)\n",
        "\n",
        "  optimizer = optax.adam(learning_rate=learning_rate)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  rnn_loss_grad = jax.value_and_grad(make_rnn_loss(model), has_aux=True)\n",
        "  opt_state = optimizer.init(params)\n",
        "  for i in range(n_epochs):\n",
        "      key, subkey = jax.random.split(key)\n",
        "      subsequence = sample_subsequence(key, encoded_data, max_subsequence_length).astype(jnp.int32)\n",
        "\n",
        "      state = model.init_state()\n",
        "      batch_losses = []\n",
        "      for inputs, targets in zip(*chunk(subsequence, batch_size)):\n",
        "        (loss, state), gradient = rnn_loss_grad(params, inputs, targets, state)\n",
        "        updates, opt_state = optimizer.update(gradient, opt_state)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        batch_losses.append(loss)\n",
        "      if not (i % 10):\n",
        "        if test_prompt is None:\n",
        "          test_prompt = data[:4]\n",
        "        generated_string = sample(key, model, params, id_lookup, chr_lookup, test_prompt, max_length=sample_length, temperature=temperature)\n",
        "        print(f\"Epoch {i} Average loss: {jnp.mean(jnp.asarray(batch_losses)):.5f} random sample: {generated_string}\") "
      ],
      "metadata": {
        "id": "k8NljkfVcm5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "We learn our language model over a children book called \"The Life and Adventures of Santa Claus\" by L. Frank Baum. "
      ],
      "metadata": {
        "id": "lnx2IpAN-kFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gutenberg.org/cache/epub/520/pg520.txt"
      ],
      "metadata": {
        "id": "zS_17yTck7eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pg520.txt', 'r') as file:\n",
        "  data = file.read()"
      ],
      "metadata": {
        "id": "Z-rbHYnsA0NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training your model, you should be able to observe that the samples become more coherent over time while the log likelihood loss goes down. "
      ],
      "metadata": {
        "id": "A7TzNdYU--EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(key, data, state_size=256, learning_rate=1e-3, batch_size=64, n_epochs=2000, max_subsequence_length=5000, sample_length=50, test_prompt='Santa ', temperature=1e-1)"
      ],
      "metadata": {
        "id": "_t2iLRP-CA3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}